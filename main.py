import time
import sys
import os
import json
import re
import argparse
from src.logger_config import logger, configure_logging
from src.firecrawl_client import FirecrawlClient
from src.processing import (
    filter_urls,
    validate_and_prepare_sources,
    score_sources,
    select_best_sources,
    clean_content,
)
from src.llm_processing import (
    extract_prompts_from_article,
    generate_article_by_sections,  # NEW: for section-by-section generation
    fact_check_sections,  # NEW: for fact-checking individual sections
    editorial_review,
    _load_and_prepare_messages,
    _make_llm_request_with_retry,
    save_llm_interaction,
    _parse_json_from_response
)
from src.wordpress_publisher import WordPressPublisher
from src.token_tracker import TokenTracker
from src.config import LLM_MODELS, FALLBACK_MODELS
from batch_config import CONTENT_TYPES, get_content_type_config
from typing import Dict

def sanitize_filename(topic):
    """Sanitizes the topic to be used as a valid directory name."""
    return re.sub(r'[\\/*?:"<>|]', "_", topic).replace(" ", "_")

def save_artifact(data, path, filename):
    """Saves data to a file (JSON or text)."""
    os.makedirs(path, exist_ok=True)
    filepath = os.path.join(path, filename)
    with open(filepath, 'w', encoding='utf-8') as f:
        if isinstance(data, str):
            f.write(data)
        else:
            json.dump(data, f, indent=4, ensure_ascii=False)
    logger.info(f"Saved artifact to {filepath}")

def fix_content_newlines(content: str) -> str:
    """
    –ò—Å–ø—Ä–∞–≤–ª—è–µ—Ç —ç–∫—Ä–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫ –≤ code –±–ª–æ–∫–∞—Ö.
    –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –ª–∏—Ç–µ—Ä–∞–ª—å–Ω—ã–µ \\n –≤ –Ω–∞—Å—Ç–æ—è—â–∏–µ –ø–µ—Ä–µ–Ω–æ—Å—ã –¢–û–õ–¨–ö–û –≤–Ω—É—Ç—Ä–∏ <pre><code> —Ç–µ–≥–æ–≤.
    """
    if not content:
        return content

    # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –±–ª–æ–∫–æ–≤ –∫–æ–¥–∞
    def fix_code_block(match):
        pre_tag = match.group(1)  # <pre> —Å –≤–æ–∑–º–æ–∂–Ω—ã–º–∏ –∞—Ç—Ä–∏–±—É—Ç–∞–º–∏
        code_opening = match.group(2)  # <code> —Å –≤–æ–∑–º–æ–∂–Ω—ã–º–∏ –∞—Ç—Ä–∏–±—É—Ç–∞–º–∏
        code_content = match.group(3)  # –°–æ–¥–µ—Ä–∂–∏–º–æ–µ –±–ª–æ–∫–∞ –∫–æ–¥–∞
        code_closing = match.group(4)  # </code>
        pre_closing = match.group(5)  # </pre>

        # –ó–∞–º–µ–Ω—è–µ–º –ª–∏—Ç–µ—Ä–∞–ª—å–Ω—ã–µ \n –Ω–∞ –Ω–∞—Å—Ç–æ—è—â–∏–µ –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫
        fixed_content = code_content.replace('\\n', '\n')

        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
        if '\\n' in code_content:
            logger.debug(f"Fixed code block: replaced {code_content.count('\\n')} \\n occurrences")

        return f"{pre_tag}{code_opening}{fixed_content}{code_closing}{pre_closing}"

    # –†–µ–≥—É–ª—è—Ä–Ω–æ–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è –ø–æ–∏—Å–∫–∞ –±–ª–æ–∫–æ–≤ <pre><code>...</code></pre>
    import re
    pattern = r'(<pre[^>]*>)(<code[^>]*>)(.*?)(</code>)(</pre>)'

    # –ó–∞–º–µ–Ω—è–µ–º –≤—Å–µ –±–ª–æ–∫–∏ –∫–æ–¥–∞
    fixed_content = re.sub(pattern, fix_code_block, content, flags=re.DOTALL)

    return fixed_content


def save_html_with_proper_newlines(content: str, path: str, filename: str):
    """
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç HTML –∫–æ–Ω—Ç–µ–Ω—Ç —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –ø–µ—Ä–µ–Ω–æ—Å–∞–º–∏ —Å—Ç—Ä–æ–∫ –≤ code –±–ª–æ–∫–∞—Ö.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–±—â—É—é —Ñ—É–Ω–∫—Ü–∏—é fix_content_newlines() –¥–ª—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø–µ—Ä–µ–Ω–æ—Å–æ–≤.
    """
    os.makedirs(path, exist_ok=True)
    filepath = os.path.join(path, filename)

    # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫
    fixed_content = fix_content_newlines(content)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    with open(filepath, 'w', encoding='utf-8') as f:
        f.write(fixed_content)

    logger.info(f"Saved HTML with proper newlines to {filepath}")

async def basic_articles_pipeline(topic: str, publish_to_wordpress: bool = True, content_type: str = "basic_articles",
                                  verbose: bool = False, variables_manager=None):
    """
    Simplified pipeline for generating basic articles with FAQ and sources.
    Improved pipeline with configurable content type for different prompt sets.
    –≠—Ç–∞–ø—ã: 1-6 –ø–æ–∏—Å–∫/–æ—á–∏—Å—Ç–∫–∞ ‚Üí 7 —Å—Ç—Ä—É–∫—Ç—É—Ä—ã ‚Üí 8 —É–ª—å—Ç–∏–º–∞—Ç–∏–≤–Ω–∞—è ‚Üí 9 –≥–µ–Ω–µ—Ä–∞—Ü–∏—è ‚Üí 9.5 —Ñ–∞–∫—Ç-—á–µ–∫ ‚Üí 10 —Ä–µ–¥–∞–∫—Ç—É—Ä–∞ ‚Üí 11 –ø—É–±–ª–∏–∫–∞—Ü–∏—è

    Args:
        topic: Topic for content generation
        publish_to_wordpress: Whether to publish to WordPress
        content_type: Type of content to generate
        verbose: Enable verbose logging
        variables_manager: Optional VariablesManager instance with variables
    """
    logger.info(f"--- Starting Basic Articles Pipeline for topic: '{topic}' ---")

    # Log active variables if any
    if variables_manager:
        active_vars = variables_manager.get_active_variables_summary()
        if active_vars['active_count'] > 0:
            logger.info(f"Active variables: {active_vars['variables']}")

    # Initialize token tracker
    token_tracker = TokenTracker(topic=topic)

    # Use default models from config
    active_models = LLM_MODELS

    # --- Setup Directories ---
    sanitized_topic = sanitize_filename(topic)
    base_output_path = os.path.join("output", sanitized_topic)
    paths = {
        "search": os.path.join(base_output_path, "01_search"),
        "parsing": os.path.join(base_output_path, "02_parsing"),
        "scoring": os.path.join(base_output_path, "03_scoring"),
        "selection": os.path.join(base_output_path, "04_selection"),
        "cleaning": os.path.join(base_output_path, "05_cleaning"),
        "structure_extraction": os.path.join(base_output_path, "06_structure_extraction"),
        "ultimate_structure": os.path.join(base_output_path, "07_ultimate_structure"),
        "final_article": os.path.join(base_output_path, "08_article_generation"),
        "fact_check": os.path.join(base_output_path, "09_fact_check"),
        "editorial_review": os.path.join(base_output_path, "10_editorial_review"),
    }
    for path in paths.values():
        os.makedirs(path, exist_ok=True)

    # --- –≠—Ç–∞–ø—ã 1-6: –ü–æ–∏—Å–∫, –ø–∞—Ä—Å–∏–Ω–≥, –æ—á–∏—Å—Ç–∫–∞ ---
    firecrawl_client = FirecrawlClient()

    search_results = await firecrawl_client.search(topic)
    save_artifact(search_results, paths["search"], "01_search_results.json")

    urls = [result['url'] for result in search_results if 'url' in result]
    if not urls:
        logger.error("No URLs found in search results. Exiting.")
        return
    save_artifact(urls, paths["search"], "02_extracted_urls.json")

    clean_urls = filter_urls(urls)
    save_artifact(clean_urls, paths["parsing"], "01_clean_urls.json")

    if not clean_urls:
        logger.error("No clean URLs left after filtering. Exiting.")
        return

    scraped_data = await firecrawl_client.scrape_urls(clean_urls)
    save_artifact(scraped_data, paths["parsing"], "02_scraped_data.json")

    valid_sources = validate_and_prepare_sources(scraped_data)
    save_artifact(valid_sources, paths["parsing"], "03_valid_sources.json")

    if not valid_sources:
        logger.error("No valid sources found after scraping and validation. Exiting.")
        return

    scored_sources = score_sources(valid_sources, topic)
    save_artifact(scored_sources, paths["scoring"], "scored_sources.json")

    top_sources = select_best_sources(scored_sources)
    save_artifact(top_sources, paths["selection"], "top_5_sources.json")

    if not top_sources:
        logger.error("Could not select any top sources. Exiting.")
        return

    cleaned_sources = clean_content(top_sources)
    save_artifact(cleaned_sources, paths["cleaning"], "final_cleaned_sources.json")

    # --- –≠—Ç–∞–ø 7: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä (–ü–ê–†–ê–õ–õ–ï–õ–¨–ù–û) ---
    logger.info(f"Starting PARALLEL structure extraction from {len(cleaned_sources)} sources...")

    def extract_all_structures():
        """Extract structures from all sources sequentially"""
        results = []

        # Process sources sequentially with delays
        for i, source in enumerate(cleaned_sources):
            source_id = f"source_{i+1}"
            logger.info(f"üöÄ Starting structure extraction for {source_id}")

            # Add delay between requests (except for first)
            if i > 0:
                delay = 5  # 5 seconds between requests
                logger.info(f"‚è≥ {source_id} waiting {delay}s before HTTP request...")
                time.sleep(delay)
                logger.info(f"‚úÖ {source_id} finished waiting, starting HTTP request...")

            try:
                result = extract_prompts_from_article(
                    article_text=source['cleaned_content'],
                    topic=topic,
                    base_path=paths["structure_extraction"],
                    source_id=source_id,
                    token_tracker=token_tracker,
                    model_name=active_models.get("extract_prompts"),
                    content_type=content_type,
                    variables_manager=variables_manager
                )
                results.append(result)
            except Exception as e:
                results.append(e)

        # Process results
        all_structures = []
        extraction_stats = []

        for i, result in enumerate(results):
            source_id = f"source_{i+1}"
            source = cleaned_sources[i]

            if isinstance(result, Exception):
                logger.error(f"‚ùå {source_id} failed with exception: {result}")
                extraction_stats.append({
                    "source_id": source_id,
                    "url": source.get('url', 'Unknown'),
                    "structures_extracted": 0,
                    "error": str(result)
                })
            else:
                structures = result
                extraction_stats.append({
                    "source_id": source_id,
                    "url": source.get('url', 'Unknown'),
                    "structures_extracted": len(structures)
                })

                if len(structures) == 0:
                    logger.warning(f"‚ö†Ô∏è  {source_id} extracted 0 structures - possible JSON parsing issue")
                else:
                    logger.info(f"‚úÖ {source_id} extracted {len(structures)} structures")

                all_structures.extend(structures)

        return all_structures, extraction_stats

    # Run the sync extraction
    all_structures, extraction_stats = extract_all_structures()

    save_artifact(all_structures, paths["structure_extraction"], "all_structures.json")

    if not all_structures:
        logger.error("No structures could be extracted from the sources. Exiting.")
        return

    # --- –≠—Ç–∞–ø 8: –°–æ–∑–¥–∞–Ω–∏–µ —É–ª—å—Ç–∏–º–∞—Ç–∏–≤–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã ---
    logger.info("Creating ultimate structure from extracted structures...")

    messages = _load_and_prepare_messages(
        content_type,
        "02_create_ultimate_structure",
        {"topic": topic, "article_text": json.dumps(all_structures, indent=2)},
        variables_manager=variables_manager,
        stage_name="create_structure"
    )

    # Try with primary model first, then fallback if JSON parsing fails
    ultimate_structure = None
    models_to_try = [
        active_models.get("create_structure"),
        FALLBACK_MODELS.get("create_structure")  # google/gemini-2.5-flash-lite-preview-06-17
    ]

    for model_idx, current_model in enumerate(models_to_try):
        if not current_model or (model_idx > 0 and current_model == models_to_try[0]):
            continue  # Skip if no fallback or same as primary

        model_label = "primary" if model_idx == 0 else "fallback"

        for attempt in range(1, 4):  # 3 attempts per model
            try:
                logger.info(f"üîÑ Create structure attempt {attempt}/3 with {model_label} model: {current_model}")

                response_obj, actual_model = _make_llm_request_with_retry(
                    stage_name="create_structure",
                    model_name=current_model,
                    messages=messages,
                    token_tracker=token_tracker,
                    base_path=paths["ultimate_structure"],
                    temperature=0.3
                )

                content = response_obj.choices[0].message.content
                save_llm_interaction(
                    base_path=paths["ultimate_structure"],
                    stage_name="create_structure",
                    messages=messages,
                    response=content,
                    request_id=f"ultimate_structure_{model_label}_attempt{attempt}"
                )

                ultimate_structure = _parse_json_from_response(content)

                if ultimate_structure and ultimate_structure != []:
                    logger.info(f"‚úÖ Successfully parsed structure with {current_model} on attempt {attempt}")
                    save_artifact(ultimate_structure, paths["ultimate_structure"], "ultimate_structure.json")
                    break
                else:
                    logger.warning(f"‚ùå Invalid JSON from {current_model} on attempt {attempt}")
                    if attempt < 3:
                        time.sleep(2)  # Small delay before retry
                    elif model_idx == 0 and models_to_try[1]:
                        logger.warning(f"üîÑ Primary model failed, switching to fallback model...")

            except Exception as e:
                logger.error(f"Error with {current_model} on attempt {attempt}: {e}")
                if attempt < 3:
                    time.sleep(2)

        if ultimate_structure:
            break

    if not ultimate_structure or ultimate_structure == []:
        logger.error("Failed to create valid structure with all models and attempts. Exiting.")
        return

    # --- –≠—Ç–∞–ø 9: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è WordPress —Å—Ç–∞—Ç—å–∏ –ø–æ —Å–µ–∫—Ü–∏—è–º ---
    logger.info("Generating WordPress-ready article from ultimate structure (section by section)...")

    # NEW: Use section-by-section generation
    wordpress_data = generate_article_by_sections(
        structure=ultimate_structure,
        topic=topic,
        base_path=paths["final_article"],
        token_tracker=token_tracker,
        model_name=active_models.get("generate_article"),
        content_type=content_type,
        variables_manager=variables_manager
    )

    save_artifact(wordpress_data, paths["final_article"], "wordpress_data.json")

    if isinstance(wordpress_data, dict) and "raw_response" in wordpress_data:
        logger.info(f"Generated article data ready for fact-checking")
    else:
        logger.error("Invalid WordPress data structure returned")
        return

    # --- –≠—Ç–∞–ø 9.5: Fact-checking —Å–µ–∫—Ü–∏–π ---
    logger.info("Starting grouped fact-checking of generated sections...")

    generated_sections = wordpress_data.get("generated_sections", [])
    if not generated_sections:
        logger.error("No generated sections found for fact-checking. Exiting.")
        return

    # Get combined fact-checked content and status
    fact_checked_content, fact_check_status = fact_check_sections(
        sections=generated_sections,
        topic=topic,
        base_path=paths["fact_check"],
        token_tracker=token_tracker,
        model_name=active_models.get("fact_check"),
        content_type=content_type,
        variables_manager=variables_manager
    )

    # Save the combined fact-checked content
    save_artifact({"content": fact_checked_content}, paths["fact_check"], "fact_checked_content.json")

    # Save fact-check status for reference
    save_artifact(fact_check_status, paths["fact_check"], "fact_check_status.json")

    # Check for fact-check failures and show warning
    fact_check_failed = not fact_check_status.get("success", True)
    if fact_check_failed:
        failed_groups = fact_check_status.get("failed_groups", 0)
        total_groups = fact_check_status.get("total_groups", 0)
        failed_sections = fact_check_status.get("failed_sections", [])

        # Display bright warning
        border = "üî•" * 60
        logger.warning(f"\n{border}")
        logger.warning(f"‚ö†Ô∏è  CRITICAL: FACT-CHECK FAILED")
        logger.warning(f"Failed groups: {failed_groups}/{total_groups}")
        if failed_sections:
            logger.warning(f"Failed sections: {', '.join(failed_sections[:5])}")  # Show first 5 sections
            if len(failed_sections) > 5:
                logger.warning(f"... and {len(failed_sections) - 5} more sections")
        logger.warning(f"Article contains UNVERIFIED CONTENT - Manual review required!")
        logger.warning(f"{border}\n")
    else:
        logger.info(f"‚úÖ Fact-checking passed: All {fact_check_status.get('total_groups', 0)} groups verified")

    # Create merged content structure for compatibility with editorial review
    merged_content = {
        "title": f"–°—Ç–∞—Ç—å—è –ø–æ —Ç–µ–º–µ: {topic}",
        "content": fact_checked_content,
        "excerpt": f"–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—Ç–∞—Ç—å—è –Ω–∞ —Ç–µ–º—É: {topic}",
        "slug": topic.lower().replace(" ", "-")
    }
    save_artifact(merged_content, paths["fact_check"], "merged_fact_checked_content.json")

    # Update wordpress_data with fact-checked content
    wordpress_data["raw_response"] = json.dumps(merged_content, ensure_ascii=False)

    logger.info(f"Fact-checking completed: Combined content length: {len(fact_checked_content)} characters")

    # --- –≠—Ç–∞–ø 10: Editorial Review ---
    logger.info("Starting editorial review and cleanup...")
    raw_response = wordpress_data.get("raw_response", "")
    wordpress_data_final = editorial_review(
        raw_response=raw_response,
        topic=topic,
        base_path=paths["editorial_review"],
        token_tracker=token_tracker,
        model_name=active_models.get("editorial_review"),
        content_type=content_type,
        variables_manager=variables_manager
    )

    # –ò—Å–ø—Ä–∞–≤–∏—Ç—å –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫ –≤ –∫–æ–Ω—Ç–µ–Ω—Ç–µ –ø–µ—Ä–µ–¥ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º JSON
    if isinstance(wordpress_data_final, dict) and "content" in wordpress_data_final:
        wordpress_data_final["content"] = fix_content_newlines(wordpress_data_final["content"])
        logger.info("Fixed newlines in wordpress_data_final content for JSON compatibility")

    save_artifact(wordpress_data_final, paths["editorial_review"], "wordpress_data_final.json")

    if isinstance(wordpress_data_final, dict) and "content" in wordpress_data_final:
        save_html_with_proper_newlines(wordpress_data_final["content"], paths["editorial_review"], "article_content_final.html")
        logger.info(f"Editorial review completed: {wordpress_data_final.get('title', 'No title')}")
    else:
        logger.warning("Editorial review returned invalid structure, using original data")
        wordpress_data_final = wordpress_data

    # --- –≠—Ç–∞–ø 11 (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π): WordPress Publication ---
    if publish_to_wordpress:
        logger.info("Starting WordPress publication...")
        try:
            wp_publisher = WordPressPublisher()

            publication_result = wp_publisher.publish_article(wordpress_data_final)

            if publication_result["success"]:
                logger.info(f"‚úÖ Article published successfully: {publication_result['url']}")
                save_artifact(publication_result, paths["editorial_review"], "wordpress_publication_result.json")
            else:
                logger.error(f"‚ùå WordPress publication failed: {publication_result.get('error', 'Unknown error')}")

        except Exception as e:
            logger.error(f"WordPress publication failed: {e}")
            save_artifact({
                "success": False,
                "error": str(e),
                "url": None
            }, paths["editorial_review"], "wordpress_publication_result.json")

    # --- Final Summary ---
    logger.info("=== PIPELINE COMPLETED ===")
    logger.info(f"Topic: {topic}")
    logger.info(f"Final article title: {wordpress_data_final.get('title', 'No title')}")

    # Show fact-check warning in final summary if needed
    if fact_check_failed:
        border = "üî•" * 60
        logger.warning(f"\n{border}")
        logger.warning(f"‚ö†Ô∏è  FINAL WARNING: Article contains UNVERIFIED CONTENT")
        logger.warning(f"Fact-check failed for {fact_check_status.get('failed_groups', 0)} groups")
        logger.warning(f"Manual fact verification recommended before publication")
        logger.warning(f"{border}\n")

    # Token usage report
    token_summary = token_tracker.get_session_summary()
    logger.info(f"Total tokens used: {token_summary['session_summary']['total_tokens']}")
    token_report_path = os.path.join(base_output_path, "token_usage_report.json")
    token_tracker.save_token_report(base_output_path)
    logger.info(f"Token usage report: {token_report_path}")

async def run_single_stage(topic: str, stage: str, content_type: str = "basic_articles", publish_to_wordpress: bool = True, verbose: bool = False):
    """
    –ó–∞–ø—É—Å–∫–∞–µ—Ç pipeline —Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —ç—Ç–∞–ø–∞, –∏—Å–ø–æ–ª—å–∑—É—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ.

    Args:
        topic: –¢–µ–º–∞ —Å—Ç–∞—Ç—å–∏ (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –ø–∞–ø–∫–∏ output)
        stage: –≠—Ç–∞–ø –¥–ª—è –∑–∞–ø—É—Å–∫–∞ ('fact_check', 'editorial_review', 'publication')
        content_type: –¢–∏–ø –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        publish_to_wordpress: –ü—É–±–ª–∏–∫–æ–≤–∞—Ç—å –ª–∏ –≤ WordPress
        verbose: –í–∫–ª—é—á–∏—Ç—å –¥–µ—Ç–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
    """
    from src.llm_processing import editorial_review
    from src.config import LLM_MODELS
    from src.token_tracker import TokenTracker

    # –ù–∞–π—Ç–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –ø–∞–ø–∫—É output
    sanitized_topic = sanitize_filename(topic)
    base_output_path = f"output/_{sanitized_topic}_"

    if not os.path.exists(base_output_path):
        logger.error(f"Output folder not found: {base_output_path}")
        logger.error("Run full pipeline first to create the necessary data files")
        return

    logger.info(f"Using existing output folder: {base_output_path}")

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
    token_tracker = TokenTracker()
    active_models = LLM_MODELS

    # –°–æ–∑–¥–∞–Ω–∏–µ –ø—É—Ç–µ–π –∫ —ç—Ç–∞–ø–∞–º
    paths = {
        "final_article": os.path.join(base_output_path, "08_article_generation"),
        "fact_check": os.path.join(base_output_path, "09_fact_check"),
        "editorial_review": os.path.join(base_output_path, "10_editorial_review")
    }

    # Initialize variables_manager for all stages
    from src.variables_manager import VariablesManager
    variables_manager = VariablesManager()

    if stage == "fact_check":
        logger.info("=== Starting Fact-Check Stage ===")

        # Load wordpress_data.json from 08_final_article
        wordpress_data_path = os.path.join(paths["final_article"], "wordpress_data.json")
        if not os.path.exists(wordpress_data_path):
            logger.error(f"Required file not found: {wordpress_data_path}")
            logger.error("Run full pipeline first to generate article sections")
            return

        with open(wordpress_data_path, 'r', encoding='utf-8') as f:
            wordpress_data = json.load(f)

        generated_sections = wordpress_data.get("generated_sections", [])
        if not generated_sections:
            logger.error("No generated sections found in wordpress_data.json")
            return

        logger.info(f"Found {len(generated_sections)} sections for fact-checking")

        # Run fact-checking
        from src.llm_processing import fact_check_sections

        fact_checked_content, fact_check_status = fact_check_sections(
            sections=generated_sections,
            topic=topic,
            base_path=paths["fact_check"],
            token_tracker=token_tracker,
            model_name=active_models.get("fact_check"),
            content_type=content_type,
            variables_manager=variables_manager
        )

        # Save results
        save_artifact({"content": fact_checked_content},
                     paths["fact_check"],
                     "fact_checked_content.json")
        save_artifact(fact_check_status,
                     paths["fact_check"],
                     "fact_check_status.json")

        # Create merged_fact_checked_content.json for editorial_review
        merged_content = {
            "title": wordpress_data.get("title", ""),
            "content": fact_checked_content,
            "excerpt": wordpress_data.get("excerpt", ""),
            "seo_title": wordpress_data.get("seo_title", ""),
            "meta_description": wordpress_data.get("meta_description", ""),
            "sources": wordpress_data.get("sources", []),
            "faq": wordpress_data.get("faq", [])
        }

        save_artifact(merged_content,
                     paths["fact_check"],
                     "merged_fact_checked_content.json")

        logger.info(f"‚úÖ Fact-check stage completed successfully")

        # Show token statistics
        token_summary = token_tracker.get_session_summary()
        logger.info(f"Tokens used in this stage: {token_summary['session_summary']['total_tokens']}")

    elif stage == "editorial_review":
        logger.info("=== Starting Editorial Review Stage ===")

        # –ó–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –ø–æ—Å–ª–µ fact-check
        merged_content_path = os.path.join(paths["fact_check"], "merged_fact_checked_content.json")
        if not os.path.exists(merged_content_path):
            logger.error(f"Required file not found: {merged_content_path}")
            return

        with open(merged_content_path, 'r', encoding='utf-8') as f:
            merged_content = json.load(f)

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –≤ —Ñ–æ—Ä–º–∞—Ç–µ, –æ–∂–∏–¥–∞–µ–º–æ–º editorial_review
        raw_response = json.dumps(merged_content, ensure_ascii=False)

        # –ó–∞–ø—É—Å—Ç–∏—Ç—å Editorial Review
        wordpress_data_final = editorial_review(
            raw_response=raw_response,
            topic=topic,
            base_path=paths["editorial_review"],
            token_tracker=token_tracker,
            model_name=active_models.get("editorial_review"),
            content_type=content_type,
            variables_manager=variables_manager
        )

        # –ò—Å–ø—Ä–∞–≤–∏—Ç—å –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫ –≤ –∫–æ–Ω—Ç–µ–Ω—Ç–µ –ø–µ—Ä–µ–¥ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º JSON
        if isinstance(wordpress_data_final, dict) and "content" in wordpress_data_final:
            wordpress_data_final["content"] = fix_content_newlines(wordpress_data_final["content"])
            logger.info("Fixed newlines in wordpress_data_final content for JSON compatibility")

        save_artifact(wordpress_data_final, paths["editorial_review"], "wordpress_data_final.json")

        if isinstance(wordpress_data_final, dict) and "content" in wordpress_data_final:
            save_html_with_proper_newlines(wordpress_data_final["content"], paths["editorial_review"], "article_content_final.html")
            logger.info(f"‚úÖ Editorial review completed: {wordpress_data_final.get('title', 'No title')}")
        else:
            logger.warning("Editorial review returned invalid structure")
            return

        logger.info(f"Editorial Review stage completed successfully")

        # –ü–æ–∫–∞–∑–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ç–æ–∫–µ–Ω–æ–≤
        token_summary = token_tracker.get_session_summary()
        logger.info(f"Tokens used in this stage: {token_summary['session_summary']['total_tokens']}")

    elif stage == "publication":
        logger.info("=== Starting WordPress Publication Stage ===")

        # –ó–∞–≥—Ä—É–∑–∏—Ç—å –≥–æ—Ç–æ–≤—ã–π wordpress_data_final.json
        wordpress_data_path = os.path.join(paths["editorial_review"], "wordpress_data_final.json")
        if not os.path.exists(wordpress_data_path):
            logger.error(f"Required file not found: {wordpress_data_path}")
            logger.error("Run editorial_review stage first to create wordpress_data_final.json")
            return

        with open(wordpress_data_path, 'r', encoding='utf-8') as f:
            wordpress_data_final = json.load(f)

        logger.info(f"Loaded WordPress data: {wordpress_data_final.get('title', 'No title')}")

        # –ü—Ä–∏–º–µ–Ω–∏—Ç—å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø–µ—Ä–µ–Ω–æ—Å–æ–≤ —Å—Ç—Ä–æ–∫ (–Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π)
        if isinstance(wordpress_data_final, dict) and "content" in wordpress_data_final:
            wordpress_data_final["content"] = fix_content_newlines(wordpress_data_final["content"])
            logger.info("Applied newline fixes to content")

        if publish_to_wordpress:
            logger.info("Starting WordPress publication...")
            try:
                from src.wordpress_publisher import WordPressPublisher
                wp_publisher = WordPressPublisher()

                publication_result = wp_publisher.publish_article(wordpress_data_final)

                if publication_result["success"]:
                    logger.info(f"‚úÖ Article published successfully: {publication_result['url']}")
                    save_artifact(publication_result, paths["editorial_review"], "wordpress_publication_result.json")
                else:
                    logger.error(f"‚ùå Publication failed: {publication_result['error']}")
                    save_artifact(publication_result, paths["editorial_review"], "wordpress_publication_error.json")

            except Exception as e:
                logger.error(f"‚ùå WordPress publication error: {e}")
                return
        else:
            logger.info("WordPress publication skipped (--skip-publication)")

        logger.info(f"Publication stage completed successfully")

    else:
        logger.error(f"Stage '{stage}' not implemented yet")
        logger.info("Available stages: fact_check, editorial_review, publication")

async def main_flow(topic: str, model_overrides: Dict = None, publish_to_wordpress: bool = True, content_type: str = "basic_articles", verbose: bool = False, variables_manager=None):
    """Async wrapper function for batch processor compatibility"""
    return await basic_articles_pipeline(topic, publish_to_wordpress, content_type, verbose, variables_manager)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Content Factory Pipeline')
    parser.add_argument('topic', help='Topic for content generation')
    parser.add_argument('--content-type', choices=list(CONTENT_TYPES.keys()),
                       default='basic_articles', help='Type of content to generate')
    parser.add_argument('--skip-publication', action='store_true',
                       help='Skip WordPress publication')
    parser.add_argument('--start-from-stage', choices=['fact_check', 'editorial_review', 'publication'],
                       help='Start pipeline from specific stage (requires existing output folder)')
    parser.add_argument('--verbose', action='store_true',
                       help='Show detailed debug logs (default: show only key events)')

    # Variable arguments
    parser.add_argument('--article-length', type=int,
                       help='Target article length in characters')
    parser.add_argument('--author-style',
                       help='Author style for writing (e.g., "academic", "conversational", "technical")')
    parser.add_argument('--theme-focus',
                       help='Theme focus for the content (e.g., "business", "technology", "education")')
    parser.add_argument('--custom-requirements',
                       help='Additional requirements for content generation')
    parser.add_argument('--target-audience',
                       help='Target audience for the article (e.g., "beginners", "professionals", "students")')
    parser.add_argument('--tone-of-voice',
                       help='Tone of voice (e.g., "formal", "friendly", "authoritative")')
    parser.add_argument('--include-examples', action='store_true',
                       help='Include practical examples in each section')
    parser.add_argument('--seo-keywords',
                       help='SEO keywords to naturally include (comma-separated)')

    args = parser.parse_args()

    # Configure logging FIRST before any other operations
    configure_logging(verbose=args.verbose)

    # Re-import logger after configuration to get updated settings
    from src.logger_config import logger

    # Validate content type
    try:
        content_config = get_content_type_config(args.content_type)
        logger.info(f"Using content type: {args.content_type} - {content_config['description']}")
    except ValueError as e:
        logger.error(f"Invalid content type: {e}")
        sys.exit(1)

    publish_to_wordpress = not args.skip_publication

    import asyncio

    # Create variables manager from CLI arguments
    from src.variables_manager import VariablesManager
    variables_manager = VariablesManager.create_from_args(vars(args))

    if variables_manager.get_active_variables_summary()["active_count"] > 0:
        logger.info(f"Variables manager initialized with {variables_manager.get_active_variables_summary()['active_count']} variable(s)")
        for var_name, var_value in variables_manager.get_active_variables_summary()["variables"].items():
            logger.info(f"  - {var_name}: {var_value}")

    # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Ñ–ª–∞–≥ --start-from-stage
    if args.start_from_stage:
        logger.info(f"Starting from stage: {args.start_from_stage}")
        logger.info(f"Topic: {args.topic}")
        logger.info(f"Content type: {args.content_type}")

        try:
            asyncio.run(run_single_stage(args.topic, args.start_from_stage, args.content_type, publish_to_wordpress, args.verbose))
            logger.info(f"‚úÖ Stage '{args.start_from_stage}' completed successfully")
        except KeyboardInterrupt:
            logger.info("\\nüõë Stage interrupted by user")
            sys.exit(130)
        except Exception as e:
            logger.error(f"üí• Stage '{args.start_from_stage}' failed: {e}")
            sys.exit(1)
    else:
        logger.info(f"Starting full pipeline for topic: {args.topic}")
        logger.info(f"Content type: {args.content_type}")
        logger.info(f"WordPress publication: {'enabled' if publish_to_wordpress else 'disabled'}")

        try:
            asyncio.run(basic_articles_pipeline(args.topic, publish_to_wordpress, args.content_type, args.verbose, variables_manager))
            logger.info("‚úÖ Pipeline completed successfully")
        except KeyboardInterrupt:
            logger.info("\\nüõë Pipeline interrupted by user")
            sys.exit(130)
        except Exception as e:
            logger.error(f"üí• Pipeline failed: {e}")
            sys.exit(1)