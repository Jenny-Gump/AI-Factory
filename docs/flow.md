# Pipeline Flow Description (Updated)

This document provides a detailed, step-by-step breakdown of the simplified content generation pipeline for WordPress article generation.

---

## ИНСТРУКЦИЯ ПО ДИАГНОСТИКЕ ПРОБЛЕМ В ПАЙПЛАЙНЕ

### Как правильно находить проблемы:

#### 1. **ВСЕГДА НАЧИНАТЬ С СЫРЫХ LLM ДАННЫХ**
- **ПЕРВЫМ ДЕЛОМ** проверяй файлы `output/{topic}/*/llm_responses_raw/*.txt`
- Смотри что **ТОЧНО** ушло в LLM и что **ТОЧНО** пришло
- **НЕ полагайся** на логи или промежуточные JSON файлы

#### 2. **СТРОГО СЛЕДОВАТЬ ЦЕПОЧКЕ ДАННЫХ**
- Проверяй **ЧТО** входит в каждый этап (input)
- Проверяй **ЧТО** выходит из каждого этапа (output)
- Найди **ТОЧНОЕ** место где данные меняются неожиданно

#### 3. **НЕ ПРЕДПОЛАГАТЬ - ПРОВЕРЯТЬ ФАКТЫ**
- **НЕ говори** "откуда-то", "возможно", "может быть"
- Читай **РЕАЛЬНЫЕ** файлы с данными
- Смотри **ФАКТИЧЕСКИЕ** запросы/ответы LLM

#### 4. **МЕТОД "ОТ КОНЦА К НАЧАЛУ"**
- Возьми финальный неправильный результат
- Иди **НАЗАД** по этапам до источника проблемы
- Найди где **ИМЕННО** происходит нежелательное изменение

#### 5. **ВСЕГДА ПРОВЕРЯТЬ ПРОМПТЫ**
- Читай промпты которые идут в LLM
- Проверяй есть ли инструкции которые могут вызвать проблему
- Смотри что LLM может неправильно понять

#### 6. **ПОРЯДОК ДИАГНОСТИКИ:**
1. Проверить финальный результат и определить что не так
2. Найти все `llm_responses_raw/*.txt` файлы в папках этапов
3. Проверить каждый ответ LLM в обратном порядке этапов
4. Найти где впервые появляется проблема
5. Проверить промпт этого этапа
6. Исправить промпт или код

**НЕ ТРАТИТЬ ВРЕМЯ НА АНАЛИЗ ЛОГИКИ КОДА БЕЗ ПРОВЕРКИ СЫРЫХ ДАННЫХ!**

---


### Этап 1: Запрос (The Request)

**ВХОДНЫЕ ДАННЫЕ:** 
- Командная строка: `python main.py "тема"`
- Аргумент topic (строка)

**ЦЕЛЬ:** Получить исходную тему для всего пайплайна

**ФУНКЦИИ:**
- `main.py` → `argparse` парсинг аргументов
- Валидация входной строки (не пустая)
- Создание базовой структуры папок `output/{тема}/`

**ПРОЦЕСС:** 
1. Пользователь вводит тему: `"Best prompts for data analysis in 2025"`
2. Система создает slug из темы для имен папок
3. Инициализируется структура директорий для артефактов

**ВЫХОДНЫЕ ДАННЫЕ:**
- `topic: str` - исходная тема
- `topic_slug: str` - нормализованное имя для папок
- Созданы папки: `output/{topic_slug}/01_search/...`

---

### Этап 2: Поиск (Search)

**ВХОДНЫЕ ДАННЫЕ:**
- `topic: str` - тема для поиска
- `FIRECRAWL_API_KEY` - API ключ из .env
- `search_limit: int = 20` - количество результатов

**ЦЕЛЬ:** Найти максимально широкий, но релевантный список URL-адресов по теме

**ФУНКЦИИ:**
- `src/firecrawl_client.py` → `search(topic)`
- Firecrawl Search API (`/v2/search`) запрос
- Обработка ошибок API и таймаутов

**ПРОЦЕСС:**
1. Формируется поисковый запрос без кавычек для широкого охвата
2. POST запрос к `https://api.firecrawl.dev/v2/search`
3. API возвращает топ-20 результатов из поисковой выдачи Google
4. Первичная валидация URL (исключение битых ссылок)

**ВЫХОДНЫЕ ДАННЫЕ:**
```json
[
  {
    "url": "https://example.com/article1",
    "title": "Best Data Analysis Prompts 2025",
    "description": "Complete guide to...",
    "favicon": "https://example.com/favicon.ico"
  }
]
```
**Артефакт:** `output/{тема}/01_search/01_search_results.json`

---

### Этап 3: Парсинг (Parsing)

**ВХОДНЫЕ ДАННЫЕ:**
- Список URL из этапа 2 (20 ссылок)
- `filters/blocked_domains.json` - черный список доменов
- `MIN_CONTENT_LENGTH = 10000` - минимальная длина статьи

**ЦЕЛЬ:** Извлечь основной контент с каждого URL и отсеять неподходящие статьи

**ФУНКЦИИ:**
- `src/processing.py` → `filter_urls()` - фильтрация по черному списку
- `src/firecrawl_client.py` → `scrape_urls()` - конкурентный парсинг
- Firecrawl Scrape API (`/v2/scrape`) с `onlyMainContent: true`
- Валидация длины контента

**ПРОЦЕСС:**
1. **Предварительная фильтрация:** Проверка каждого URL по `blocked_domains.json`
2. **Конкурентное извлечение:** Параллельные запросы к Firecrawl Scrape API
3. **Валидация контента:** Проверка минимальной длины (10000+ символов)
4. **Очистка данных:** Удаление статей с пустым или коротким контентом

**ВЫХОДНЫЕ ДАННЫЕ:**
```json
[
  {
    "url": "https://example.com/article1",
    "title": "Complete Guide to Data Analysis",
    "content": "# Introduction\n\nData analysis has become..." // Markdown
  }
]
```
**Артефакты:**
- `01_clean_urls.json` - URL после фильтрации
- `02_scraped_data.json` - сырые данные от Firecrawl  
- `03_valid_sources.json` - валидированные источники

---

### Этап 4: Оценка (Scoring)

**ВХОДНЫЕ ДАННЫЕ:**
- Валидные источники из этапа 3
- `filters/trusted_sources.json` - белый список доменов
- `topic` - ключевые слова для релевантности
- Веса из `src/config.py`: TRUST_WEIGHT, RELEVANCE_WEIGHT, DEPTH_WEIGHT

**ЦЕЛЬ:** Присвоить каждому источнику объективную трехмерную оценку качества

**ФУНКЦИИ:**
- `src/processing.py` → `calculate_trust_score()` - проверка доверенных доменов
- `src/processing.py` → `calculate_relevance_score()` - анализ ключевых слов
- `src/processing.py` → `calculate_depth_score()` - оценка глубины контента
- Токенизация и анализ текста

**ПРОЦЕСС:**
1. **Трастовость:** Проверка домена по `trusted_sources.json` → (1.0-2.5)
2. **Релевантность:** Подсчет вхождений ключевых слов:
   - В заголовке: вес × 3
   - В тексте: стандартный вес
3. **Глубина:** Длина статьи в символах → нормализация к (0-1)
4. **Агрегация:** Сохранение всех метрик для следующего этапа

**ВЫХОДНЫЕ ДАННЫЕ:**
```json
[
  {
    "url": "https://example.com/article1",
    "title": "Complete Guide",
    "content": "...",
    "trust_score": 2.0,      // 1.0-2.5
    "relevance_score": 0.8,  // 0.0-1.0
    "depth_score": 0.9       // 0.0-1.0
  }
]
```
**Артефакт:** `output/{тема}/03_scoring/scored_sources.json`

---

### Этап 5: Отбор (Selection)

**ВХОДНЫЕ ДАННЫЕ:**
- Оцененные источники с метриками (trust, relevance, depth)
- Веса из config: `TRUST_WEIGHT=0.5`, `RELEVANCE_WEIGHT=0.3`, `DEPTH_WEIGHT=0.2`
- `TOP_SOURCES_COUNT = 5` - количество финалистов

**ЦЕЛЬ:** Выбрать 5 "чемпионов" - лучших источников для LLM анализа

**ФУНКЦИИ:**
- `src/processing.py` → `normalize_scores()` - приведение к единой шкале
- `src/processing.py` → `calculate_final_score()` - взвешенная формула
- `src/processing.py` → `select_top_sources()` - отбор финалистов
- Сортировка по финальному баллу

**ПРОЦЕСС:**
1. **Нормализация:** Все оценки приводятся к шкале (0-1)
2. **Взвешенная формула:** 
   ```python
   Final_Score = (trust * 0.5) + (relevance * 0.3) + (depth * 0.2)
   ```
3. **Сортировка:** Весь список по `Final_Score` (убывание)
4. **Отбор топ-5:** Берутся лучшие источники для LLM этапов

**ВЫХОДНЫЕ ДАННЫЕ:**
```json
[
  {
    "url": "https://example.com/article1",
    "title": "Best Guide",
    "content": "...",
    "trust_score": 2.0,
    "relevance_score": 0.9,
    "depth_score": 0.8,
    "final_score": 1.57,    // Взвешенная сумма
    "rank": 1               // Позиция в рейтинге
  }
]
```
**Артефакт:** `output/{тема}/04_selection/top_5_sources.json`

---

### Этап 6: Очистка (Cleaning)

**ВХОДНЫЕ ДАННЫЕ:**
- Топ-5 источников с "грязным" Markdown контентом
- Паттерны очистки в `src/processing.py`
- Список нежелательных элементов (навигация, реклама)

**ЦЕЛЬ:** Удалить семантический "мусор" и подготовить чистый текст для LLM

**ФУНКЦИИ:**
- `src/processing.py` → `clean_content()` - основная очистка
- `remove_navigation_elements()` - удаление навигации
- `clean_markdown_artifacts()` - очистка разметки
- `normalize_whitespace()` - нормализация пробелов
- Регулярные выражения для паттернов мусора

**ПРОЦЕСС:**
1. **Удаление навигации:** Меню, breadcrumbs, "Related articles"
2. **Очистка рекламы:** Banner текст, партнерские ссылки
3. **Markdown артефакты:** Битая разметка, лишние символы
4. **Нормализация:** Множественные переносы → одинарные
5. **Валидация:** Проверка что основной контент сохранен

**ВЫХОДНЫЕ ДАННЫЕ:**
```json
[
  {
    "url": "https://example.com/article1",
    "title": "Best Guide",
    "cleaned_content": "# Data Analysis Prompts\n\nHere are the most effective..." // Чистый Markdown
  }
]
```
**Артефакт:**
- `final_cleaned_sources.json` - JSON со всеми очищенными источниками

---

### Этап 7: Извлечение структур (Structure Extraction)

**ВХОДНЫЕ ДАННЫЕ:**
- Топ-5 очищенных источников из этапа 6 (`final_cleaned_sources.json`)
- `content_type="basic_articles"`
- Промпт `prompts/basic_articles/01_extract.txt`
- Папка вывода: `06_structure_extraction/`

**ЦЕЛЬ:** Извлечь структурные схемы из каждого источника - разделы, их назначение, фокус контента

**ФУНКЦИИ:**
- `extract_prompts_from_article()` с параметром `content_type="basic_articles"`
- LLM анализ через `basic_articles/01_extract.txt` промпт
- Создание JSON схем для каждого источника

**ПРОЦЕСС:**
1. **Итерация по источникам:** Каждый из 5 источников (`source_1`, `source_2`, etc.)
2. **LLM структурный анализ:** Анализ организации контента, H2/H3 заголовков
3. **Извлечение схемы:** JSON с полями `section_title`, `section_description`, `content_focus`, `key_points`, `subsections`
4. **Агрегация:** Все структуры собираются в общий массив `all_structures`
5. **Логирование:** Статистика по каждому источнику (`structures_extracted`)

**ВЫХОДНЫЕ ДАННЫЕ:**
```json
[
  {
    "section_title": "Основы технологии",
    "section_description": "Объяснение базовых принципов и концепций",
    "content_focus": "Образовательный контент для новичков",
    "key_points": "Определения, принципы работы, основные преимущества",
    "subsections": ["Что такое технология", "Принципы работы"]
  }
]
```

**Артефакт:** `06_structure_extraction/all_structures.json`!!!

---

### Этап 8: Создание ультимативной структуры (Ultimate Structure Creation) 🏗️

**ВХОДНЫЕ ДАННЫЕ:**
- `all_structures.json` из этапа 7 (структуры от 5 источников)
- Промпт `prompts/basic_articles/02_create_ultimate_structure.txt`
- `topic` из командной строки для тематической фокусировки
- FREE DeepSeek Chat v3.1 API credentials (via OpenRouter)

**🎯 ЦЕЛЬ:** Объединить 5 структур в единую схему статьи с FAQ и разделом Источники

**ФУНКЦИИ:**
- LLM анализ через функцию создания структуры в `src/llm_processing.py`
- FREE DeepSeek с fallback на Gemini 2.5 при таймаутах
- Синтез структур с добавлением FAQ раздела
- Подготовка схемы для посекционной генерации

**ПРОЦЕСС:**
1. **Анализ структур:** LLM получает все 5 извлеченных структур
2. **Синтез:** Создание единой, наиболее полной структуры статьи
3. **Добавление FAQ:** Автоматическое планирование раздела с частыми вопросами
4. **Добавление Источников:** Планирование раздела с нумерованными ссылками [1]-[5]
5. **Оптимизация:** Логическая последовательность разделов для читаемости

**ВЫХОДНЫЕ ДАННЫЕ:**
```json
[{
  "article_structure": [
    {
      "section_title": "Введение",
      "section_description": "Обзор темы и её важности",
      "subsections": ["Определение", "Актуальность"]
    },
    {
      "section_title": "Основные концепции",
      "section_description": "Ключевые понятия и принципы",
      "subsections": ["Базовые принципы", "Методология"]
    },
    {
      "section_title": "FAQ",
      "section_description": "Часто задаваемые вопросы с развернутыми ответами",
      "subsections": ["Вопрос 1", "Вопрос 2", "Вопрос 3"]
    },
    {
      "section_title": "Полезные ссылки",
      "section_description": "Нумерованные источники для углубленного изучения",
      "subsections": ["Источник 1", "Источник 2", "Источник 3"]
    }
  ]
}]
```

**Артефакты:**
- `ultimate_structure.json` - единая структура статьи для генерации
- `llm_requests/create_structure_request.json` - запрос к LLM
- `llm_responses_raw/create_structure_response.txt` - сырой ответ LLM

**КЛЮЧЕВЫЕ ОСОБЕННОСТИ:**
- **Синтез лучшего:** Объединяет сильные стороны всех 5 источников
- **FAQ интеграция:** Автоматическое планирование интерактивных вопросов
- **Источники:** Подготовка для нумерованных ссылок на источники
- **Логическая структура:** Оптимальная последовательность разделов
- **Подготовка к генерации:** Готовая схема для посекционной обработки

---

### Этап 9: Посекционная генерация статьи (Section-by-Section Generation) 🤖

**ВХОДНЫЕ ДАННЫЕ:**
- `ultimate_structure.json` из этапа 8
- Промпт `prompts/basic_articles/01_generate_section.txt`
- `topic` из командной строки для персонализации
- FREE DeepSeek Chat v3.1 API credentials (via OpenRouter)

**🎯 ЦЕЛЬ:** Сгенерировать статью по секциям с задержками против rate limits

**ФУНКЦИИ:**
- `src/llm_processing.py` → `generate_article_by_sections()` - главная функция
- Парсинг `structure[0]["article_structure"]` из ultimate structure
- FREE DeepSeek с fallback на Gemini 2.5 при таймаутах
- 5-секундные паузы между секциями для избежания rate limiting

**ПРОЦЕСС:**
1. **Парсинг структуры:** Извлечение секций из `ultimate_structure.json`
2. **Посекционная генерация:** Для каждой секции отдельно:
   - Отправка промпта с `{topic}`, `{section_title}`, `{section_structure}`
   - LLM генерирует HTML контент для конкретной секции
   - Сохранение в `sections/section_N/`
   - Пауза 5 секунд перед следующей секцией
3. **Объединение:** Сборка всех секций в единую статью
4. **Создание метаданных:** Формирование WordPress структуры

**ВЫХОДНЫЕ ДАННЫЕ:**
```json
{
  "title": "Название статьи на основе темы",
  "content": "<h2>Введение</h2><p>Контент первой секции...</p><h2>Основные концепции</h2><p>Контент второй секции...</p><details><summary>Частый вопрос 1?</summary><p>Развернутый ответ...</p></details>",
  "excerpt": "Краткое описание статьи",
  "slug": "url-friendly-slug",
  "categories": ["basic-articles"],
  "_yoast_wpseo_title": "SEO заголовок",
  "_yoast_wpseo_metadesc": "SEO описание",
  "focus_keyword": "ключевое слово"
}
```

**Артефакты:**
- `sections/section_1/` до `section_N/` - каждая секция отдельно
  - `llm_requests/generate_section_request.json`
  - `llm_responses_raw/generate_section_response.txt`
- `merged_content.json` - объединенные секции
- `wordpress_data.json` - финальная структура для WordPress

**КЛЮЧЕВЫЕ ОСОБЕННОСТИ:**
- **Избежание rate limits:** 5-секундные паузы между запросами
- **Детальная диагностика:** Каждая секция логируется отдельно
- **Retry отдельных секций:** При ошибке можно повторить только проблемную секцию
- **FAQ интеграция:** Автоматическое создание `<details><summary>` блоков
- **Fallback система:** DeepSeek timeout → автоматический Gemini 2.5
- **Структурированность:** Каждая секция имеет свою папку с логами

---

### Этап 10: Редакторская обработка (Editorial Review) ✏️

**ВХОДНЫЕ ДАННЫЕ:**
- Черновая структура `wordpress_data.json` из этапа 9
- `topic` пользователя для тематической фокусировки
- Промпт `prompts/basic_articles/02_editorial_review.txt`
- FREE DeepSeek Chat v3.1 API credentials (via OpenRouter)

**🎯 ЦЕЛЬ:** Профессиональная редактура и очистка статьи для финальной публикации

**ФУНКЦИИ:**
- `src/llm_processing.py` → `editorial_review()` - главная функция редакторской обработки
- FREE DeepSeek с низкой температурой (0.2) для стабильного редактирования
- Робастный JSON парсинг с обработкой ошибок

**ПРОЦЕСС:**
1. **Загрузка промпта:** Специальный промпт для редактора с передачей `{topic}`
2. **LLM редактура:** FREE DeepSeek выполняет:
   - Улучшение структуры контента (таблицы, списки, абзацы)
   - **Тематическую фокусировку:** Сохранение наиболее полезного контента по теме
   - Редакторский контроль текста (удаление "воды" и общих фраз)
   - Улучшение заголовков и метаданных
   - Оптимизация FAQ форматирования
3. **JSON валидация:** Многослойная проверка и парсинг результата

**ВЫХОДНЫЕ ДАННЫЕ:**
```json
{
  "title": "Отредактированный заголовок",
  "content": "<h2>Улучшенный контент</h2><p>Отформатированный текст...</p>",
  "excerpt": "Привлекательное описание статьи",
  "slug": "optimized-url-slug",
  "_yoast_wpseo_title": "SEO-оптимизированный заголовок",
  "_yoast_wpseo_metadesc": "Интригующее мета-описание"
}
```

**Артефакты:**
- `wordpress_data_final.json` - финальная отредактированная статья
- `article_content_final.html` - чистый HTML контент
- `llm_requests/editorial_review_request.json`
- `llm_responses_raw/editorial_review_response.txt`

---

### Этап 10.5: Link Processing (Обработка ссылок)

**ВХОДНЫЕ ДАННЫЕ:**
- Финальная отредактированная структура `wordpress_data_final.json` из этапа 10
- HTML контент статьи для анализа
- Промпт `prompts/basic_articles/01_5_link_planning.txt`
- Firecrawl Search API для поиска кандидатов

**🎯 ЦЕЛЬ:** Добавить авторитетные внешние ссылки в статью с академическим форматированием и разделом "Источники"

**ФУНКЦИИ:**
- `src/link_processor.py` → `LinkProcessor()` - главный класс
- `create_link_plan()` - создание плана ссылок по позициям символов
- `search_candidates()` - поиск кандидатов через Firecrawl
- `select_links()` - выбор лучших ссылок с приоритетом качества
- `apply_links()` - вставка маркеров и создание раздела "Источники"

**ПРОЦЕСС:**
1. **Анализ контента:** LLM анализирует HTML и определяет позиции символов для ссылок (10-20 штук)
2. **Создание плана:** Возвращается JSON с позициями:
   ```json
   {
     "link_plan": [{
       "ref_id": "1",
       "query": "Least-to-Most prompting arxiv paper",
       "character_position": 1234,
       "hint": "Research paper on LTM",
       "section": "Теоретические основы"
     }]
   }
   ```
3. **Умная вставка маркеров:** Автоматическая коррекция позиций:
   - Поиск ближайшего подходящего места (после тега, точки, запятой)
   - Избегание разрыва слов и предложений
   - Вставка академических маркеров `<a id="cite-N">[N]</a>`
4. **Поиск кандидатов:** Параллельный поиск через Firecrawl API
5. **Отбор ссылок:** Приоритет академических источников:
   - docs.*, developer.* (официальная документация)
   - arxiv.org, papers.nips.cc (научные статьи)
   - github.com (репозитории)
   - Блокировка: medium.com, reddit.com, stackoverflow.com
6. **Создание раздела "Полезные ссылки":** Форматированный список источников в конце

**ВЫХОДНЫЕ ДАННЫЕ:**
```json
{
  "title": "...",
  "content": "<h2>Введение</h2><p>LTM промптинг[1] это метод...</p>...<h2>Источники</h2><ol><li id='ref-1'>Least-to-Most Prompting Paper - <a href='https://arxiv.org/abs/2205.10625'>arxiv.org</a></li></ol>",
  "excerpt": "...",
  // остальные поля без изменений
}
```

**Артефакты:**
- `link_plan.json` - план с позициями символов (10-20 ссылок)
- `candidates.json` - найденные кандидаты по запросам
- `selected_links.json` - отобранные ссылки для вставки
- `draft_with_markers.html` - статья с расставленными маркерами
- `article_with_links.html` - финальная статья со ссылками
- `links_report.json` - отчет об успешности (обычно 90-95%)

**КЛЮЧЕВЫЕ ОСОБЕННОСТИ:**
- **Увеличенное количество ссылок:** 10-20 вместо 5-12 для лучшего SEO
- **Умное позиционирование:** Автоматическая коррекция позиций для естественного размещения
- **Академический приоритет:** Фокус на официальные и научные источники
- **Защита от JSON ошибок:** Позиции вместо HTML = меньше данных = стабильность
- **Маленький JSON:** Только план ссылок, без большого HTML контента
- **Приоритет качества:** Академические источники > документация > GitHub
- **Автоматическая нумерация:** Последовательные маркеры [1], [2], [3]
- **Раздел "Источники":** Автоматическое создание в конце статьи
- **Обратные ссылки:** Клик по источнику ведет к месту в тексте

---

### Этап 11: WordPress Publication (WordPress Publication) 🌐

**ВХОДНЫЕ ДАННЫЕ:**
- Финальная структура с ссылками из этапа 10.5
- WordPress креденции из `.env` файла (PetrovA, app password)
- Флаг `--publish-wp` из командной строки
- WordPress API настройки

**🎯 ЦЕЛЬ:** Автоматически опубликовать статью на https://ailynx.ru в категории "prompts" в статусе черновика с полной поддержкой Yoast SEO

**ФУНКЦИИ:**
- `src/wordpress_publisher.py` → `WordPressPublisher()` - главный класс
- `publish_article()` - основная функция публикации
- `test_wordpress_connection()` - тестирование соединения
- WordPress REST API v2 интеграция
- Автоматическое управление категориями

**ПРОЦЕСС:**
1. **Тестирование соединения:** Проверка WordPress API и аутентификации
2. **Подготовка данных:** Адаптация JSON структуры для WordPress REST API
3. **Обработка категорий:** Поиск или создание категории "prompts" (ID: 825)
4. **Публикация статьи:** POST запрос к `/wp-json/wp/v2/posts` с полными данными
5. **SEO метаданные:** Автоматическая установка Yoast SEO полей через meta
6. **Сохранение результата:** Логирование WordPress ID и URL для редактирования

**ВЫХОДНЫЕ ДАННЫЕ:**
```json
{
  "success": true,
  "wordpress_id": 4377,
  "url": "https://ailynx.ru/wp-admin/post.php?post=4377&action=edit",
  "error": null
}
```

**Артефакты:**
- `wordpress_publication_result.json` - результат публикации с WordPress ID
- Логи процесса публикации в основном лог-файле

**КЛЮЧЕВЫЕ ОСОБЕННОСТИ:**
- **Опциональность:** Активируется только с флагом `--publish-wp`
- **Безопасность:** Статьи создаются в статусе `draft` для проверки
- **SEO интеграция:** Автоматическое заполнение всех Yoast SEO полей
- **Категоризация:** Все статьи автоматически попадают в категорию "prompts"
- **Валидация:** Полная проверка соединения и креденций перед публикацией
- **Обработка ошибок:** Детальное логирование и graceful fallback при сбоях
- **Аккаунт PetrovA:** Специальный аккаунт для Content Factory публикаций

---

## 🔧 **Техническая архитектура и управление (ОБНОВЛЕНО)**

### **Запуск пайплайна:**
```bash
# Полный пайплайн с генерацией и редактурой (этапы 1-8)
python main.py "Лучшие промпты для создания идей для видео"

# Полный пайплайн с публикацией (этапы 1-9)
python main.py "Промпты для анализа данных" --publish-wp

# Кастомные модели с публикацией
python main.py "ChatGPT промпты для маркетинга" --generate-model openai/gpt-4o --publish-wp

# Создание категории WordPress (одноразово)
python create_prompts_category.py
```

### **Упрощенный поток данных (basic_articles):**
```
Этапы 1-5: Поиск, очистка контента (те же что в prompt_collection)
           ↓
Этап 6: Извлечение структур из 5 источников → all_structures.json
           ↓
Этап 7: all_structures → ultimate_structure.json (создание единой структуры)
           ↓
Этап 8: ultimate_structure + topic → wordpress_data.json (черновая статья)
           ↓
Этап 9: wordpress_data.json → editorial_review() → wordpress_data_final.json (чистая статья)
           ↓
Этап 10: wordpress_data_final.json → link_processing() → wordpress_data_with_links.json (+ ссылки и источники)
           ↓
Этап 11: wordpress_data_with_links.json → WordPress API → draft post (опционально)
```

### **Обработка ошибок:**
- **Робастный JSON парсинг** с 4 стратегиями fallback
- **Полное логирование запросов/ответов** для отладки
- **Graceful degradation** при сбоях LLM вызовов
- **Валидация этапов** перед переходом к следующему

### **Оптимизация производительности:**
- **Конкурентный скрапинг** (настройка через CONCURRENT_REQUESTS)
- **Умное кэширование** через сохранение артефактов
- **Токен оптимизация** (200 для примеров, 600 для комментариев)
- **Прогрессивное улучшение** (каждый этап развивает предыдущий)

### **Ключевые принципы дизайна (ОБНОВЛЕНО):**
1. **Простота и эффективность**: Полный 9-этапный пайплайн с обязательной редактурой
2. **WordPress-ориентированность**: Прямая генерация и автоматическая публикация контента
3. **Качественное улучшение промптов**: LLM дорабатывает найденные промпты до идеального состояния
4. **Профессиональная редактура**: Этап 8 обеспечивает качество публикации редактором-ИИ
5. **Полная прозрачность**: Комплексное логирование всех LLM и WordPress взаимодействий
6. **Русскоязычный фокус**: Специализация на российскую аудиторию и сайт ailynx.ru
7. **100% FREE-powered**: FREE DeepSeek models обеспечивают высококачественную генерацию и редактуру с нулевой стоимостью
8. **SEO-оптимизация**: Автоматическое заполнение Yoast SEO полей для поисковых систем
9. **Безопасная публикация**: Статьи создаются в статусе draft для проверки перед публикацией
10. **Практическая ценность**: Фокус на реальных случаях использования и экспертных рекомендациях

Эта полная 9-этапная архитектура обеспечивает быструю генерацию, профессиональную редактуру и автоматическую публикацию высококачественных WordPress статей о коллекциях промптов с полной готовностью к использованию.

---

## 🔍 **Система сохранения сырых ответов LLM (ДОБАВЛЕНО: Сентябрь 2025)**

### **Назначение**
Автоматическое сохранение всех сырых ответов от LLM API для отладки проблем с JSON парсингом, особенно ошибок типа "Expecting value: line 5241 column 1 (char 28820)" от DeepSeek API.

### **Структура сохранения**
Каждый этап пайплайна сохраняет сырые ответы в свою папку:

```
output/{topic}/
├── 06_structure_extraction/
│   └── llm_responses_raw/
│       ├── extract_prompts_response_attempt1_20250920_164532.txt
│       ├── extract_prompts_response_attempt2_20250920_164545.txt
│       └── ERROR_extract_prompts_response_attempt3_20250920_164558.txt
├── 07_ultimate_structure/
│   └── llm_responses_raw/
│       └── create_structure_response_attempt1_20250920_164612.txt
├── 08_article_generation/
│   └── sections/
│       ├── section_1/llm_responses_raw/
│       │   └── generate_article_response_attempt1_20250920_164625.txt
│       ├── section_2/llm_responses_raw/
│       └── ...
├── 09_editorial_review/
│   └── llm_responses_raw/
│       ├── ERROR_editorial_review_response_attempt1_20250920_164638.txt
│       └── editorial_review_response_attempt2_20250920_164651.txt
└── 10_link_processing/
    └── llm_responses_raw/
        ├── link_planning_response_attempt1_20250920_164704.txt
        └── link_selection_response_attempt1_20250920_164717.txt
```

### **Именование файлов**

#### **Успешные ответы:**
```
{stage_name}_response_attempt{N}_{timestamp}.txt
```
- `stage_name`: extract_prompts, create_structure, generate_article, editorial_review, link_planning, link_selection
- `attempt{N}`: номер попытки (1, 2, 3)
- `timestamp`: YYYYMMDD_HHMMSS

#### **Ошибочные ответы:**
```
ERROR_{stage_name}_response_attempt{N}_{timestamp}.txt
```
Сохраняются когда LLM API вернул ответ, но произошла ошибка парсинга JSON.

### **Формат содержимого файла**
```
TIMESTAMP: 2025-09-20T16:45:32.123456
MODEL: deepseek/deepseek-chat-v3.1:free
STAGE: editorial_review
ATTEMPT: 1
RESPONSE_LENGTH: 28820
SUCCESS: False
ERROR: Expecting value: line 5241 column 1 (char 28820)
================================================================================
{
  "title": "Улучшенный заголовок статьи",
  "content": "<h2>Введение</h2><p>Контент статьи...</p>
  // ЗДЕСЬ СЫРОЙ ОТВЕТ ОБРЫВАЕТСЯ ИЛИ СОДЕРЖИТ ПОВРЕЖДЕННЫЙ JSON
```

### **Применение для отладки**

#### **При ошибках JSON парсинга:**
1. Найти файл `ERROR_*` в папке проблемного этапа
2. Открыть файл и увидеть точное содержимое ответа LLM
3. Проанализировать где именно обрывается JSON или что вызывает ошибку
4. Понять причину: обрыв соединения, лимиты API, некорректный JSON от модели

#### **Пример типичных проблем:**
- **Обрыв соединения:** Ответ обрывается посередине JSON объекта
- **Превышение лимитов:** API возвращает HTML страницу с ошибкой вместо JSON
- **Некорректный JSON:** LLM генерирует невалидный JSON с неэкранированными кавычками
- **Кодировка:** Проблемы с UTF-8 символами в ответе

### **Технические детали**

#### **Автоматичность:**
- Система работает прозрачно, не требует дополнительной настройки
- Файлы создаются автоматически при каждом LLM запросе
- Не влияет на производительность (запись файлов в фоне)

#### **Место в коде:**
- Функция `_make_llm_request_with_retry_sync()` в `src/llm_processing.py`
- Сохранение происходит до парсинга JSON, сразу после получения ответа от API
- Поддерживается во всех LLM функциях: retry, timeout, sync, async

#### **Совместимость:**
- Работает со всеми этапами пайплайна
- Поддерживает все модели: DeepSeek, OpenRouter, Gemini
- Совместимо с существующей системой `save_llm_interaction()`

### **Преимущества**
1. **Полная прозрачность:** Видно точно что возвращает API
2. **Быстрая отладка:** Мгновенное понимание причины JSON ошибок
3. **Историчность:** Сохраняются все попытки для анализа паттернов
4. **Автономность:** Работает без дополнительных действий разработчика
5. **Структурированность:** Файлы организованы по этапам и времени

---

## TROUBLESHOOTING

### Частые проблемы и решения

#### 1. **Ошибка парсинга JSON от LLM**
**Симптомы:** `json.decoder.JSONDecodeError`
**Решение:**
- Проверьте `output/{topic}/*/llm_responses_raw/*.txt`
- Убедитесь что промпт требует JSON формат
- Проверьте fallback модели в config.py

#### 2. **Таймаут при парсинге сайтов**
**Симптомы:** Зависание на этапе 2 (Parse)
**Решение:**
- Увеличьте таймаут в src/config.py: `SECTION_TIMEOUT = 300`
- Проверьте блокировки на Firecrawl API

#### 3. **Недостаточно источников после фильтрации**
**Симптомы:** "После фильтрации осталось 0 источников"
**Решение:**
- Проверьте `filters/blocked_domains.json`
- Уменьшите `MIN_CONTENT_LENGTH` в config.py
- Расширьте поисковый запрос

#### 4. **Ошибка публикации в WordPress**
**Симптомы:** 401 Unauthorized или 403 Forbidden
**Решение:**
- Проверьте `WORDPRESS_APP_PASSWORD` в .env
- Убедитесь что Application Passwords включены
- Проверьте права пользователя

#### 5. **Превышен лимит токенов**
**Симптомы:** "Request too large" от API
**Решение:**
- Уменьшите `TOP_N_SOURCES` до 3-4
- Сократите промпты в `prompts/{type}/`
- Используйте более компактные модели

#### 6. **Дублирование контента в секциях**
**Симптомы:** Повторяющийся текст в разных секциях
**Решение:**
- Проверьте промпты генерации в `prompts/{type}/generate_article.md`
- Убедитесь что каждая секция имеет уникальный фокус

### Полезные команды для отладки

```bash
# Проверить последние ошибки
grep -r "ERROR" output/*/logs/

# Найти все неудачные LLM запросы
find output -name "*attempt[2-9]*" -type f

# Проверить размеры контента
find output -name "cleaned_content.txt" -exec wc -l {} \;

# Валидация JSON ответов
python -m json.tool output/{topic}/*/llm_responses_raw/*.txt

# Тест соединения с WordPress
curl -u "PetrovA:APP_PASSWORD" https://ailynx.ru/wp-json/wp/v2/posts
```

### Логи и диагностика

**Основные логи:**
- `output/{topic}/logs/pipeline.log` - общий лог пайплайна
- `output/{topic}/*/llm_responses_raw/` - сырые ответы LLM
- `batch_failed_topics.txt` - список неудачных тем (batch mode)

**Переменные окружения для отладки:**
```bash
export DEBUG_MODE=true        # Подробное логирование
export SKIP_WORDPRESS=true    # Пропустить публикацию
export USE_FALLBACK_ONLY=true # Использовать только fallback модели
```
